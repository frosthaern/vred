{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05309c-3a7e-4a15-b6a7-d32453c6f5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e142f",
   "metadata": {},
   "source": [
    "## loading the dataset and converting to standard types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5c9ac0-0c60-4bf3-98ed-b8e87f9bad3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "595814\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"train/eth.csv\")\n",
    "df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
    "df = df.sort_values(by=\"timestamp\").reset_index(drop=true)\n",
    "df.interpolate(inplace=true)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc81c1c",
   "metadata": {},
   "source": [
    "## feature engineering\n",
    "- adding new features and deciding which feature really corellates to the volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9467aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# may have to add a few more\n",
    "\n",
    "\n",
    "# spread\n",
    "df[\"spread\"] = df[\"ask_price1\"] - df[\"bid_price1\"]\n",
    "\n",
    "# mid price change\n",
    "df[\"mid_price_change\"] = df[\"mid_price\"].diff()\n",
    "\n",
    "# imbalance\n",
    "bid_volume_sum = df[[f\"bid_volume{i}\" for i in range(1, 6)]].sum(axis=1)\n",
    "ask_volume_sum = df[[f\"ask_volume{i}\" for i in range(1, 6)]].sum(axis=1)\n",
    "df[\"imbalance\"] = (bid_volume_sum - ask_volume_sum) / (bid_volume_sum + ask_volume_sum)\n",
    "\n",
    "# bid weighted average price\n",
    "df[\"bid_dwap\"] = (\n",
    "    df[[f\"bid_price{i}\" for i in range(1, 6)]].values\n",
    "    * df[[f\"bid_volume{i}\" for i in range(1, 6)]]\n",
    ").sum(axis=1)\n",
    "\n",
    "# ask weighted average price\n",
    "df[\"ask_dwap\"] = (\n",
    "    df[[f\"ask_price{i}\" for i in range(1, 6)]].values\n",
    "    * df[[f\"ask_volume{i}\" for i in range(1, 6)]]\n",
    ").sum(axis=1)\n",
    "\n",
    "# rolling volume\n",
    "df[\"rolling_vol\"] = df[\"mid_price\"].rolling(window=5).std()\n",
    "\n",
    "# 3. Spread-Weighted Liquidity\n",
    "near_liq_bid = sum(\n",
    "    df[f\"bid_volume{i}\"] / (df[\"mid_price\"] - df[f\"bid_price{i}\"]) for i in range(1, 6)\n",
    ")\n",
    "near_liq_ask = sum(\n",
    "    df[f\"ask_volume{i}\"] / (df[f\"ask_price{i}\"] - df[\"mid_price\"]) for i in range(1, 6)\n",
    ")\n",
    "df[\"Near_Liquidity\"] = near_liq_bid + near_liq_ask\n",
    "\n",
    "# 4. Microprice\n",
    "df[\"Microprice\"] = (\n",
    "    df[\"ask_price1\"] * df[\"bid_volume1\"] + df[\"bid_price1\"] * df[\"ask_volume1\"]\n",
    ") / (df[\"bid_volume1\"] + df[\"ask_volume1\"])\n",
    "\n",
    "# 5. Order Book Slope\n",
    "df[\"Bid_Slope\"] = (df[\"bid_price5\"] - df[\"bid_price1\"]) / bid_volume_sum\n",
    "df[\"Ask_Slope\"] = (df[\"ask_price5\"] - df[\"ask_price1\"]) / ask_volume_sum\n",
    "\n",
    "# 6. Short-Term Imbalance Change\n",
    "df[\"Delta_OBI\"] = df[\"imbalance\"].diff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86015db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_COL = [\"spread\", \"mid_price_change\", \"imbalance\", \"mid_price\", \"bid_dwap\", \"ask_dwap\", \"rolling_vol\", \"Near_Liquidity\", \"Microprice\", \"Bid_Slope\", \"Ask_Slope\", \"Delta_OBI\"]\n",
    "X_COL = [\n",
    "    \"spread\",\n",
    "    \"mid_price_change\",\n",
    "    \"imbalance\",\n",
    "    \"mid_price\",\n",
    "    \"bid_dwap\",\n",
    "    \"ask_dwap\",\n",
    "    \"rolling_vol\",\n",
    "    \"Near_Liquidity\",\n",
    "    \"Microprice\",\n",
    "    \"Bid_Slope\",\n",
    "    \"Ask_Slope\",\n",
    "    \"Delta_OBI\",\n",
    "]\n",
    "TARGET_COL = \"label\"\n",
    "X = df[X_COL]\n",
    "y = df[TARGET_COL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bee6502",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# For demonstration, let's assume you already have X, y\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Scale features\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m scaler_X \u001b[38;5;241m=\u001b[39m \u001b[43msk\u001b[49m\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mMinMaxScaler()\n\u001b[1;32m      4\u001b[0m scaler_y \u001b[38;5;241m=\u001b[39m sk\u001b[38;5;241m.\u001b[39mpreprocessing\u001b[38;5;241m.\u001b[39mMinMaxScaler()\n\u001b[1;32m      6\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler_X\u001b[38;5;241m.\u001b[39mfit_transform(X)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sk' is not defined"
     ]
    }
   ],
   "source": [
    "# For demonstration, let's assume you already have X, y\n",
    "# Scale features\n",
    "scaler_X = sk.preprocessing.MinMaxScaler()\n",
    "scaler_y = sk.preprocessing.MinMaxScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = sk.model_selection.train_test_split(\n",
    "    X_scaled, y_scaled, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# Reshape for LSTM: (samples, timesteps, features)\n",
    "# Here timesteps=1 because we treat each row as a single step\n",
    "X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "\n",
    "# Build LSTM model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.LSTM(64, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test, y_test),\n",
    ")\n",
    "\n",
    "# Predict\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_true = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = np.mean(np.abs(y_pred - y_true))\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e129e0a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a2b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c82d3ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2debce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb96030f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0298418a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a732756e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b5eea3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b6295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69160969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105fd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e350950",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
